{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9610c1-af21-4a6e-a802-2d4b50b5aff3",
   "metadata": {},
   "source": [
    "# Bank credit scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc1122-1ad3-4c92-8d27-4f943eed6314",
   "metadata": {},
   "source": [
    "Scoring is a system used by banks to evaluate clients, based on statistical methods. As a rule, it is an algorithm where the data of a potential borrower is entered. In response, a result is given - whether it is worth granting him a loan.\n",
    "\n",
    "The **task** is to implement logic to train a model for assessing the trustworthiness of bank customers. Use the accuracy metric for the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6bedd8-fe58-4bab-b88e-1897d995287f",
   "metadata": {},
   "source": [
    "**The structure of the source data:**\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| client_id | Unique identifier of the client |\n",
    "| age       | Client's age at the time of review |\n",
    "| sex       | Gender of the client |\n",
    "| married   | Marital status |\n",
    "| salary    | Official and confirmed salary of the client |\n",
    "| successful_credit_completed | Number of repaid credits |\n",
    "| credit_completed_amount | Total amount of paid credits |\n",
    "| active_credits | Number of active credits |\n",
    "| active_credits_amount | Total amount of active credits |\n",
    "| credit_amount | Amount of credits |\n",
    "| is_credit_closed | Indicator indicating whether the credit was successfully closed (paid to the bank)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60167614-b049-4187-a51e-e9551f57e92e",
   "metadata": {},
   "source": [
    "Connect the spark session, pipelines, and different transformers from the spark module: StringIndexer, VectorAssembler, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1a3cd1-42e7-44c8-9741-73c6e07821ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e118c6-3006-4be1-aa1f-3046dd48dcf2",
   "metadata": {},
   "source": [
    "Creating a Spark Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089363e5-a319-4745-8f51-4bbbdede36b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/nfs/env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/12 10:47:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"PySparkBank\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae852012-6574-4cc6-9dbc-0893a8a55615",
   "metadata": {},
   "source": [
    "We see that it is running in local mode - we do not have a master cluster. We will use the notebook's own resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e83adf-f23b-45e0-a534-437bd8e6e8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-e-2dkrupkina-2d3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkBank</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f35b0510b20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cec0c1-36a9-405b-9e0c-c58d3b59b5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bank_df = spark.read.parquet('train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e624603-2f05-4439-afee-d0d7fd482074",
   "metadata": {},
   "source": [
    "Let's look at the data. The target feature is_credit_closed, i.e. whether the credit issued to the client was successfully closed (paid to the bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c775d217-f2b1-4c31-bfca-373b67e3ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+-------+------+-----------------------------+-----------------------+--------------+---------------------+-------------+----------------+\n",
      "|           client_id|age| sex|married|salary|successfully_credit_completed|credit_completed_amount|active_credits|active_credits_amount|credit_amount|is_credit_closed|\n",
      "+--------------------+---+----+-------+------+-----------------------------+-----------------------+--------------+---------------------+-------------+----------------+\n",
      "|000070b1e6d44001b...| 29|male|  false| 11000|                            0|                      0|             0|                    0|       600000|               0|\n",
      "|000561d9b7774179a...| 36|male|   true| 70000|                            0|                      0|             0|                    0|      4450000|               1|\n",
      "|0005f6d032444a75a...| 37|male|  false| 70000|                            0|                      0|             0|                    0|      4250000|               0|\n",
      "|00061ae3663346e1b...| 53|male|  false| 70000|                            0|                      0|             0|                    0|      4300000|               0|\n",
      "|000660e148e84b56a...| 43|male|  false| 60000|                            0|                      0|             0|                    0|      2350000|               1|\n",
      "+--------------------+---+----+-------+------+-----------------------------+-----------------------+--------------+---------------------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5dac13-4026-4649-99d5-930d1e45cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = bank_df.withColumn(\"is_married\", bank_df.married.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482a553-953f-4e9d-ac5b-6967b53b9024",
   "metadata": {},
   "source": [
    "For the model to be able to use this data, we need to prepare it. First, we work with the categorical attributes - to encode the is_married column. To do this, we use StringIndexer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ec5e71-c681-49c8-abe3-f23a86360252",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_index = StringIndexer(inputCol='sex', outputCol=\"Sex_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48cf23-de8b-454b-b349-8e1f014a368f",
   "metadata": {},
   "source": [
    "We call the fit operation on our df, which will result in a transformer and we can call the transform function to transform our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e488ed-94a5-4764-8cbf-da9a7a9cd997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bank_df = sex_index.fit(bank_df).transform(bank_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "220c1c06-9155-4edd-89aa-1dd457967125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+-------+------+-----------------------------+-----------------------+--------------+---------------------+-------------+----------------+----------+---------+\n",
      "|           client_id|age| sex|married|salary|successfully_credit_completed|credit_completed_amount|active_credits|active_credits_amount|credit_amount|is_credit_closed|is_married|Sex_index|\n",
      "+--------------------+---+----+-------+------+-----------------------------+-----------------------+--------------+---------------------+-------------+----------------+----------+---------+\n",
      "|000070b1e6d44001b...| 29|male|  false| 11000|                            0|                      0|             0|                    0|       600000|               0|         0|      0.0|\n",
      "|000561d9b7774179a...| 36|male|   true| 70000|                            0|                      0|             0|                    0|      4450000|               1|         1|      0.0|\n",
      "|0005f6d032444a75a...| 37|male|  false| 70000|                            0|                      0|             0|                    0|      4250000|               0|         0|      0.0|\n",
      "|00061ae3663346e1b...| 53|male|  false| 70000|                            0|                      0|             0|                    0|      4300000|               0|         0|      0.0|\n",
      "|000660e148e84b56a...| 43|male|  false| 60000|                            0|                      0|             0|                    0|      2350000|               1|         0|      0.0|\n",
      "+--------------------+---+----+-------+------+-----------------------------+-----------------------+--------------+---------------------+-------------+----------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8eff70-5969-43b2-b6ca-0e620e801f8b",
   "metadata": {},
   "source": [
    "Now we have to do vectorization, since the models in the spark work with embeddings, and we need to vectorize the data so they can be applied to the models.\n",
    "\n",
    "First, we form a list of features to be used in model training. We form an array, place it in the features variable, specify columns to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34d61e39-a1f3-4430-9eff-c89bbab6a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age', 'salary', 'successfully_credit_completed', \n",
    "            'credit_completed_amount', 'active_credits', \n",
    "            'active_credits_amount', 'credit_amount', 'is_married', 'Sex_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e9bebb-c4a5-4262-a29b-58a8d4f7e9c2",
   "metadata": {},
   "source": [
    "Apply VectorAssembler, where we pass the inputCols parameter to the list of features columns. We assign outputCol, which is called features by default.\n",
    "\n",
    "Since VectorAssembler is a transformer, we call its transform method to apply it to our df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e383775-8792-4de4-a30f-7b68363f47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "training_data= feature.transform(bank_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d78b6-8d24-4835-8f30-ca3fa6e7f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = spark.read.parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17dd9563-1a0a-41b4-a1d3-0baf0022c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn(\"is_married\", test_df.married.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24eace33-a9d5-4100-a3bc-ed4e5cc80fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = sex_index.fit(test_df).transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7411ff73-2095-469f-9a94-d0e459e02fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = feature.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cd49b-a8c8-41e6-84da-301fc51c83fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837a1d2-1fab-466b-aed8-2c6f0a539499",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f00ad4-a808-443b-ad4f-fa55c3817a70",
   "metadata": {},
   "source": [
    "We want to create a logistic regression model to solve the classification problem. In this case logistic regression suits us, because we have only 2 classes, which we predict (2 possible values - either the loan is paid or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f0ba8-34a7-452f-9cf5-98385d3750d8",
   "metadata": {},
   "source": [
    "To evaluate models, we first need to create an evaluator (MulticlassClassificationEvaluator). We specify the column where the true value is stored in order to use it for comparison - labelCon with the feature we want to predict. Then we specify the column where the prediction from our model will be stored. The  metric of our choice is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ee40f6-d13e-427d-a2e8-614c23051112",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"is_credit_closed\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a128219c-f025-4fd6-9d92-31ead39cbce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 10:55:17 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/04/12 10:55:17 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------------+\n",
      "|prediction|is_credit_closed|            features|\n",
      "+----------+----------------+--------------------+\n",
      "|       1.0|               1|(9,[0,1,6,7],[40....|\n",
      "|       0.0|               0|(9,[0,1,6],[41.0,...|\n",
      "|       1.0|               1|(9,[0,1,6,7],[52....|\n",
      "|       1.0|               1|(9,[0,1,6,7],[44....|\n",
      "|       0.0|               1|(9,[0,1,6],[46.0,...|\n",
      "+----------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"is_credit_closed\", featuresCol=\"features\")\n",
    "\n",
    "lrModel = lr.fit(training_data)\n",
    "lr_prediction = lrModel.transform(test_data)\n",
    "lr_prediction.select(\"prediction\", \"is_credit_closed\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b427123-5d31-47f4-bc4f-bed44f70d39a",
   "metadata": {},
   "source": [
    "To apply the estimator and get an estimate from the model, we have to refer to the previously created Evaluator, call the evaluate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ededb1d-9059-4f40-9237-4a750de32ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression [Accuracy] = 0.809644\n",
      "LogisticRegression [Error] = 0.190356 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_accuracy = evaluator.evaluate(lr_prediction)\n",
    "print(\"LogisticRegression [Accuracy] = %g\"% (lr_accuracy))\n",
    "print(\"LogisticRegression [Error] = %g \" % (1.0 - lr_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d98ca7-a7ca-4c71-8ff5-b58fabbc7e01",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1c8f30-9195-40ea-b1b6-0e97ccf891f9",
   "metadata": {},
   "source": [
    "Let's experiment with other models.\n",
    "\n",
    "Let's use DecisionTreeClassifier - a decision tree. We will take it from the same classification package. We'll set the same parameters and train it in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dd580e0-53d9-4550-81ec-44b205e3f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------------+\n",
      "|prediction|is_credit_closed|            features|\n",
      "+----------+----------------+--------------------+\n",
      "|       1.0|               1|(9,[0,1,6,7],[40....|\n",
      "|       1.0|               0|(9,[0,1,6],[41.0,...|\n",
      "|       1.0|               1|(9,[0,1,6,7],[52....|\n",
      "|       1.0|               1|(9,[0,1,6,7],[44....|\n",
      "|       1.0|               1|(9,[0,1,6],[46.0,...|\n",
      "+----------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"is_credit_closed\", featuresCol=\"features\")\n",
    "dt_model = dt.fit(training_data)\n",
    "dt_prediction = dt_model.transform(test_data)\n",
    "\n",
    "dt_prediction.select(\"prediction\", \"is_credit_closed\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74872291-f997-4269-aa53-9ca4bc2a1ca9",
   "metadata": {},
   "source": [
    "We estimate the accuracy. We see that this model is more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cd1ee70-f1e2-4a9d-a545-c0ec1b107136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier [Accuracy] = 0.810827\n",
      "DecisionTreeClassifier [Error] = 0.189173 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dt_accuracy = evaluator.evaluate(dt_prediction)\n",
    "print(\"DecisionTreeClassifier [Accuracy] = %g\"% (dt_accuracy))\n",
    "print(\"DecisionTreeClassifier [Error] = %g \" % (1.0 - dt_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4911ef-b9ad-4978-a260-d111aff60a42",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f01183a-b951-485a-a3ed-483aa16a6319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------------+\n",
      "|prediction|is_credit_closed|            features|\n",
      "+----------+----------------+--------------------+\n",
      "|       1.0|               1|(9,[0,1,6,7],[40....|\n",
      "|       0.0|               0|(9,[0,1,6],[41.0,...|\n",
      "|       1.0|               1|(9,[0,1,6,7],[52....|\n",
      "|       1.0|               1|(9,[0,1,6,7],[44....|\n",
      "|       1.0|               1|(9,[0,1,6],[46.0,...|\n",
      "+----------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"is_credit_closed\", featuresCol=\"features\")\n",
    "rf_model = rf.fit(training_data)\n",
    "rf_prediction = rf_model.transform(test_data)\n",
    "rf_prediction.select(\"prediction\", \"is_credit_closed\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cafb071-fc2f-4054-ab80-c6a09dd2ab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier [Accuracy] = 0.904116\n",
      "RandomForestClassifier [Error] = 0.0958841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf_accuracy = evaluator.evaluate(rf_prediction)\n",
    "print(\"RandomForestClassifier [Accuracy] = %g\"% (rf_accuracy))\n",
    "print(\"RandomForestClassifier [Error] = %g\" % (1.0 - rf_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad5950-6d4e-4140-8dce-2b111f55ada4",
   "metadata": {},
   "source": [
    "## Gradient-boosted tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdf3e297-629c-4eea-80fe-3ad05373bf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------------+\n",
      "|prediction|is_credit_closed|            features|\n",
      "+----------+----------------+--------------------+\n",
      "|       1.0|               1|(9,[0,1,6,7],[40....|\n",
      "|       0.0|               0|(9,[0,1,6],[41.0,...|\n",
      "|       1.0|               1|(9,[0,1,6,7],[52....|\n",
      "|       1.0|               1|(9,[0,1,6,7],[44....|\n",
      "|       1.0|               1|(9,[0,1,6],[46.0,...|\n",
      "+----------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"is_credit_closed\", featuresCol=\"features\",maxIter=10)\n",
    "gbt_model = gbt.fit(training_data)\n",
    "gbt_prediction = gbt_model.transform(test_data)\n",
    "gbt_prediction.select(\"prediction\", \"is_credit_closed\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ceb855-fbd3-4ccd-bfeb-aece6f8366da",
   "metadata": {},
   "source": [
    "Having evaluated the model, we found out that it's the most accurate one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7925ee51-bca0-4a09-a908-210904283e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-boosted [Accuracy] = 0.909649\n",
      "Gradient-boosted [Error] = 0.0903505\n"
     ]
    }
   ],
   "source": [
    "gbt_accuracy = evaluator.evaluate(gbt_prediction)\n",
    "print(\"Gradient-boosted [Accuracy] = %g\"% (gbt_accuracy))\n",
    "print(\"Gradient-boosted [Error] = %g\"% (1.0 - gbt_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b02307-b4c7-49a3-b62f-d450a5432664",
   "metadata": {},
   "source": [
    "Doing a model swap is easy enough, because all sparkML models out of the box to solve regression and classification problems work through vectorization, which makes it easy enough to change the model and experiment - we have a common pipeline of what needs to be done with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860236d-c351-4675-a1b5-695db9929d89",
   "metadata": {},
   "source": [
    "## Save & Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f594813-3ee4-404e-bfb1-0c0d8b2a7397",
   "metadata": {},
   "source": [
    "Once we have trained the model, we can save it. To do this, we can use the save method of the model, which allows us to reset it to the hard disk or to some file system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0304f603-1c1d-40cd-bca6-69f7c479f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.write().overwrite().save('rf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "755047f6-f410-4272-a2b1-f26956f626c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.RandomForestClassificationModel"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "type(RandomForestClassificationModel.load('rf_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1c874f-2af7-48fd-8d44-88712edf7f09",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaeacf2-7f7b-4d26-81d4-ad63ed228706",
   "metadata": {},
   "source": [
    "Pipeline is a key thing to simplify the process of implementing models in the required environment. Let's see how it's done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee5b106a-da4b-4226-97f1-fdce116ac30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a275ce6c-519e-4c9f-85e9-73741548634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.parquet('train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dae206e4-34e6-4231-aad5-a0100e043218",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23028a62-f8de-46e5-b104-ec3692d461aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"is_married\", train.married.cast(IntegerType()))\n",
    "test = test.withColumn(\"is_married\", test.married.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a9229-5cab-4270-90cd-0104e5b4e147",
   "metadata": {},
   "source": [
    "StringIndexer is created, but not applied immediately. We create VectorAssembler, but don't apply it to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0811d1bf-063e-4e3f-a5db-c6d0a66a2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_index = StringIndexer(inputCol='sex', outputCol=\"Sex_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b59839bc-b1c4-47a8-8d43-65eadb272c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = VectorAssembler(inputCols=['age', 'salary', 'successfully_credit_completed', \n",
    "            'credit_completed_amount', 'active_credits', \n",
    "            'active_credits_amount', 'credit_amount', 'is_married', 'Sex_index'],\n",
    "                          outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243524a-d771-48e8-a9b5-b0a8ecefc72c",
   "metadata": {},
   "source": [
    "Choose the model we want to train. Specify a column with the target feature we want to predict, and a column with vectors from the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b4c78d2-5df9-4396-8ac8-0649334bc4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(labelCol=\"is_credit_closed\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf4b36-fa1c-48e5-a6bc-13848bcbac86",
   "metadata": {},
   "source": [
    "After we have defined the model, we create a pipeline:\n",
    "\n",
    "indexer for gender-> vector-assembler to build the features -> model to be trained on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cd8893e-dcbc-40a9-bc0c-7335723a2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[sex_index, feature, rf_classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73455788-79bc-4f40-9beb-8f9295faeac3",
   "metadata": {},
   "source": [
    "Once we have assembled the pipelayer, we can train it. Since it is an estimator, we can call the fit method and pass our training dataset to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "191b9b73-51bf-44ef-8d54-0676e2345205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "p_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec357837-9551-4e53-8e9f-9404b040af3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d92a3f8-9943-464f-9f56-70d7e5415677",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model.write().overwrite().save('p_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d2846a8-0fb1-43cb-9846-17cf0c8fe476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = PipelineModel.load('p_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f53ea-b42c-4f5e-a2d7-659bc8a571a3",
   "metadata": {},
   "source": [
    "Let's specify a test dataset. It does not contain the model we need for the columns. Our model will apply the transformations one by one and as a result of this transformation we will get a more complete df for the model to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8c19f44-390f-46d5-b0a1-9ce01d311659",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = p_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3ddf6c2-5639-4b37-8b88-ca1608b9e8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+-------+------+-----------------------------+-----------------------+--------------+---------------------+-------------+----------------+----------+\n",
      "|           client_id|age| sex|married|salary|successfully_credit_completed|credit_completed_amount|active_credits|active_credits_amount|credit_amount|is_credit_closed|is_married|\n",
      "+--------------------+---+----+-------+------+-----------------------------+-----------------------+--------------+---------------------+-------------+----------------+----------+\n",
      "|0000726b27784ee6b...| 40|male|   true| 90000|                            0|                      0|             0|                    0|      4350000|               1|         1|\n",
      "|0001fe9b70a44611a...| 41|male|  false| 60000|                            0|                      0|             0|                    0|      3300000|               0|         0|\n",
      "|000405b412d64b0bb...| 52|male|   true| 90000|                            0|                      0|             0|                    0|      4100000|               1|         1|\n",
      "|000b711c99db415f8...| 44|male|   true| 90000|                            0|                      0|             0|                    0|      5850000|               1|         1|\n",
      "|000e8893c2f74c55a...| 46|male|  false| 70000|                            0|                      0|             0|                    0|      1850000|               1|         0|\n",
      "+--------------------+---+----+-------+------+-----------------------------+-----------------------+--------------+---------------------+-------------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e5c6fdd-4ce9-4504-8416-56d2a2a4dfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------------+\n",
      "|prediction|is_credit_closed|            features|\n",
      "+----------+----------------+--------------------+\n",
      "|       1.0|               1|(9,[0,1,6,7],[40....|\n",
      "|       0.0|               0|(9,[0,1,6],[41.0,...|\n",
      "|       1.0|               1|(9,[0,1,6,7],[52....|\n",
      "|       1.0|               1|(9,[0,1,6,7],[44....|\n",
      "|       1.0|               1|(9,[0,1,6],[46.0,...|\n",
      "+----------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select(\"prediction\", \"is_credit_closed\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a67e04b4-0f75-4ef8-80a2-42be0eb7250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- married: boolean (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- successfully_credit_completed: integer (nullable = true)\n",
      " |-- credit_completed_amount: integer (nullable = true)\n",
      " |-- active_credits: integer (nullable = true)\n",
      " |-- active_credits_amount: integer (nullable = true)\n",
      " |-- credit_amount: integer (nullable = true)\n",
      " |-- is_credit_closed: integer (nullable = true)\n",
      " |-- is_married: integer (nullable = true)\n",
      " |-- Sex_index: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41ac5171-306e-463d-9235-879d931ccf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"is_credit_closed\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c38e628f-a98e-4764-82c3-73b4660f9055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline model [Accuracy] = 0.904116\n",
      "Pipeline model [Error] = 0.0958841 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "p_accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Pipeline model [Accuracy] = %g\"% (p_accuracy))\n",
    "print(\"Pipeline model [Error] = %g \" % (1.0 - p_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c50d3-b5e1-4bd6-b36f-9e9cea776dbd",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16fd6b-c77c-49db-aa4a-d4f234976563",
   "metadata": {},
   "source": [
    "Optimization of hyperparameters. This requires the ParamGridBuilder component and the TrainValidationSplit optimization algorithm. Select the hyperparameters to improve the quality of the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "688bcfe0-be8e-430e-a343-649be91ea3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b024c-87dd-4cd0-b71c-5f989ac30638",
   "metadata": {},
   "source": [
    "We need to set the grid of parameters that will be used to build various combinations, and then our model will be built based on these combinations. We specify the object that we want to optimize, the hyperparameter that we want to optimize, and the range of values from which the combinations are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c984f6a4-2553-467a-aed5-2604cf76f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "                  .addGrid(rf_classifier.maxDepth, [2, 3, 4])\\\n",
    "                  .addGrid(rf_classifier.maxBins, [4, 5, 6])\\\n",
    "                  .addGrid(rf_classifier.minInfoGain, [0.05, 0.1, 0.15])\\\n",
    "                  .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367cdb1-1f82-4dde-b146-c7985904970a",
   "metadata": {},
   "source": [
    "Declare TrainValidationSplit, where we specify estimator - the model to be trained (in this case we use pipeline), then specify paramMaps, from which combinations of hyperparameters will be made to train the model. Here we also need the estimator we created earlier, and specify trainRatio - the proportion of how we will split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f0b27ab-4862-465b-b5be-0ca15c904654",
   "metadata": {},
   "outputs": [],
   "source": [
    " tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=evaluator,\n",
    "                            trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2dc0f-7baf-4f48-9234-12545dbce2b3",
   "metadata": {},
   "source": [
    "We can start the learning process of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcfa88b3-0f98-4e08-8f8b-361bd8570a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d5f6d2-74f2-42a8-a45b-9b4870b2f3e7",
   "metadata": {},
   "source": [
    "As a result we get a model, but this model has TrainValidationSplitModel type, ie it is already trained TrainValidationSplit, which by default will apply the best model. But we don't need all of it, we want to use the best model.\n",
    "\n",
    "Let's turn to the parameter bestModel, to get the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82378e3c-ecd6-4af0-b0f8-9034ffe5bd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.tuning.TrainValidationSplitModel"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff022491-d76e-4f25-9fee-ecfddd5a01c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_e20c465df05d"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "459b95ff-e5b3-47ac-b280-af8e95a83efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.bestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04feea12-1d00-468a-87ea-676ed629a7ae",
   "metadata": {},
   "source": [
    "Let's see what hyperparameters were obtained in the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ed88f4c-54db-4c43-8fd3-d916807e7666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 2\n",
      "Num Trees: 4\n",
      "Impurity: 0.05\n"
     ]
    }
   ],
   "source": [
    "jo = model.bestModel.stages[-1]._java_obj\n",
    "print('Max Depth: {}'.format(jo.getMaxDepth()))\n",
    "print('Num Trees: {}'.format(jo.getMaxBins()))\n",
    "print('Impurity: {}'.format(jo.getMinInfoGain()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e26b2-5d26-4d0d-8862-59779cabf575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
